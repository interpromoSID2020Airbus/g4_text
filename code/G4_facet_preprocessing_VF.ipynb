{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Created on 16 January 2020\n",
    "    Group 4\n",
    "    Authors : Facet group\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import scipy\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {}\n",
    "with open(\"../data/glove.twitter.27B.200d.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split(' ')\n",
    "        dico[values[0]] = np.array(values[1:]).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read data\n",
    "\n",
    "def database(file, column, extension='xlsx'):\n",
    "    \"\"\"Function importing data from xlsx file and returning its comments\n",
    "    \n",
    "    Parameters:\n",
    "        file : file to import (xlsx spreadsheet)\n",
    "        column : column name of the comments you want to get from your file\n",
    "        \n",
    "    Attributes:\n",
    "        data : array containing all data from your file\n",
    "        sentences : array containing comments from your file that are located to the column\n",
    "                    and the number (index) associated to each comment in your file\n",
    "        \n",
    "    Out:\n",
    "        sentences\n",
    "    \"\"\"\n",
    "    if extension == 'xlsx':\n",
    "        data = pd.read_excel(\"../data/\" + file + \".xlsx\") # import data\n",
    "    elif extension == 'csv':\n",
    "        data = pd.read_csv(\"../data/\" + file + \".csv\", sep=',') # import data\n",
    "    else:\n",
    "        print('This function does not take into account this type of file, \\\n",
    "               be sure to use a csv or xlsx format')\n",
    "    data['index'] = range(len(data))\n",
    "    data= data.drop_duplicates(subset=[column], keep='first')\n",
    "    sentences = data.loc[:,lambda data: [\"index\", column]]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To clean data\n",
    "\n",
    "def delete_url(sentence):\n",
    "    \"\"\" \n",
    "    This function deletes url from comments\n",
    "    Parameters: \n",
    "        sentences\n",
    "    \n",
    "    Out\n",
    "        sentences without url\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', str(sentence))\n",
    "\n",
    "def delete_points(sentence):\n",
    "    \"\"\"\n",
    "    Delete Points positifs or negatifs from comments (to avoid a wrong langage detection)\n",
    "    Parameters: \n",
    "        sentences\n",
    "    \n",
    "    Out\n",
    "        sentences without points**\n",
    "    \"\"\"\n",
    "    return sentence.replace('Points positifs', ' ').replace('Points négatifs', ' ')\n",
    "\n",
    "def delete_verified(sentence):\n",
    "    \"\"\"\n",
    "    This function deletes Verified from comments\n",
    "    Parameters: \n",
    "        sentences\n",
    "    \n",
    "    Out\n",
    "        sentences without *verified*\n",
    "    \n",
    "    \"\"\"\n",
    "    return sentence.replace('Trip Verified', ' ').replace('Not Verified',' ').replace('Verified Review',' ')\n",
    "\n",
    "def treating_hashtag(sentence):\n",
    "    \"\"\"\n",
    "    Double the words following hashtag from comments\n",
    "    Parameters: \n",
    "        sentences\n",
    "    \n",
    "    Out\n",
    "        sentences without hashtag\n",
    "    \"\"\"\n",
    "    return sentence + ' ' + ' '.join(re.findall(r\"#(\\w+)\", str(sentence)))\n",
    "\n",
    "def treating_at_sign(sentence):\n",
    "    \"\"\"\n",
    "    Double the words following at sign from comments\n",
    "    Parameters: \n",
    "        sentences\n",
    "    \n",
    "    Out\n",
    "        sentences without at sign\n",
    "    \"\"\"\n",
    "    return sentence + ' ' + ' '.join(re.findall(r\"#(\\w+)\", str(sentence)))\n",
    "\n",
    "def clean_data_apply(sentence, nlp_en):\n",
    "    sentence = delete_points(sentence) # deleting \"points négatifs et positifs\" from comments\n",
    "    sentence = delete_verified(sentence) # deleting \"Trip Verified/Not Verified\" and \"Verified Review\" from comments\n",
    "    sentence = delete_url(sentence) # deleting all url/ hypelink from comments\n",
    "    sentence = treating_hashtag(sentence) # doubling hashtags in every comments to add them importance\n",
    "    sentence = treating_at_sign(sentence) # doubling @ (twitter / instagram username) to add them importance\n",
    "    sentence = strip_punctuation(sentence) # delete punctuation\n",
    "    clean_sentence = ''\n",
    "    comments = nlp_en(sentence.lower()) # lowering comments\n",
    "    if len(comments) != 0: # checking if current comment is not empty\n",
    "        try :\n",
    "            if detect(str(comments)) == 'en': # checking if the current comment is in English\n",
    "                for token in comments:\n",
    "                    clean_sentence = clean_sentence + token.lemma_ + ' ' # cleaning comments (infinitive verbs and root words)\n",
    "                clean_sentence = clean_sentence.replace('-PRON-', '') # deleting pronoun from comments\n",
    "                clean_sentence = remove_stopwords(strip_short(clean_sentence)) # deleting short words\n",
    "        except :\n",
    "            print('Comment not English')\n",
    "    else:\n",
    "        print('Comment empty')\n",
    "    \n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_apply(col_com, file, mindf = 5, maxdf = 0.7):\n",
    "    \"\"\"Function creating the tf-idf matrix from cleaned sentences and creating a csv file\n",
    "\n",
    "    Parameters:\n",
    "    col_com : pandas Series containing cleaned comments\n",
    "    file : name of your csv file\n",
    "\n",
    "    Attributes:\n",
    "    vectorizer : sklearn function computing tf idf\n",
    "    X : creation of tf-idf matrix\n",
    "    M : transfroming X in dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\",min_df=mindf, max_df=maxdf) # selecting set of words\n",
    "    X = vectorizer.fit_transform(col_com) #creating tf-idf matrix\n",
    "\n",
    "    M = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names()) # transforming in data frame\n",
    "    M.set_index(col_com.index).to_csv(\"../data/g4_tfidf_\" + file + \".csv\", sep = \",\") # saving matrix\n",
    "    \n",
    "    print(file + ' TF-IDF Matrix saved')\n",
    "\n",
    "    \n",
    "def create_tf(sentences_clean, file):\n",
    "    \"\"\"Function creating the tf matrix from cleaned sentences and creating a csv file\n",
    "\n",
    "    Parameters:\n",
    "    sentences_clean : array containing cleaned comments\n",
    "    and the number associated to each comment\n",
    "    file : name given to your csv file\n",
    "\n",
    "    Attributes:\n",
    "    comments : comments from the sentences_clean array\n",
    "    index : indexes from the sentences_clean array\n",
    "    vectorizer : set of all words\n",
    "    X : creation of tf-idf matrix\n",
    "    M : transfroming X in dataframe\n",
    "    tfidf : tf-idf matrix with cleaned comments\n",
    "    col : new column names\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    comments = [i[1] for i in sentences_clean]\n",
    "    index = [i[0] for i in sentences_clean]  \n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\",min_df=5, max_df=0.7) # selecting set of words\n",
    "    X = vectorizer.fit_transform(comments) #creating tf-idf matrix\n",
    "\n",
    "    M = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names()) # transforming in data frame\n",
    "\n",
    "    tfidf = np.concatenate((pd.DataFrame(index), pd.DataFrame(comments), M), axis=1) # add comments to tfidf\n",
    "    col = vectorizer.get_feature_names()\n",
    "    col = ['index','commentaire'] + col # renaming columns\n",
    "\n",
    "    pd.DataFrame(tfidf, columns = col).set_index('index').to_csv(\"../data/g4_tf_\" + file + \".csv\", sep = \",\") # saving matrix\n",
    "    \n",
    "    print(file + ' TF Matrix saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_apply(file, column = 'REVIEW', extension = 'xlsx'):\n",
    "    \"\"\"Function main\n",
    "    \n",
    "    Parameters:\n",
    "        file : name of your xlsx file you want to use\n",
    "        column : name of the column containing comments from your file (default = REVIEW)\n",
    "    \"\"\"\n",
    "    sentences = database(file, column, extension)\n",
    "    sentences.dropna(subset = [column], inplace = True)\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "    sentences['data_clean'] = sentences.apply(lambda x: clean_data_apply(x.loc[column], nlp_en), axis = 1)\n",
    "    pd.DataFrame(sentences['data_clean']).to_csv('../data/clean_sentences_'+file+'.csv', sep = ',')\n",
    "    #create_tfidf_apply(sentences['data_clean'], file)\n",
    "\n",
    "    \n",
    "def TF(file, column = 'REVIEW'):\n",
    "    \"\"\"Function main\n",
    "    \n",
    "    Parameters:\n",
    "        file : name of your xlsx file you want to use\n",
    "        column : name of the column containing comments from your file (default = REVIEW)\n",
    "    \"\"\"\n",
    "    sentences = database(file, column)\n",
    "    sentences_clean = clean_data(sentences, column)\n",
    "    create_tf(sentences_clean, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To cut comment into sentences\n",
    "\n",
    "def detect_sentences(comment):\n",
    "    \"\"\"Function to cut the entire comment into sentences\n",
    "    \n",
    "    Parameters : \n",
    "        comment : a comment you want to be sliced into sentences\n",
    "        \n",
    "    Attributes :\n",
    "        tokenizer : english model to be able to cut the comment\n",
    "        sentences : list of sentences of the comment\n",
    "        \n",
    "    Out : sentences\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(comment)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def duplicate_row(df, column_name):\n",
    "    \n",
    "    \"\"\"Function which duplicates comments times its number of sentences and associates them with each of the sentences\n",
    "    \n",
    "    Parameters : \n",
    "        df : matrix containing all comments imported from xlsx\n",
    "        \n",
    "    Attributes :\n",
    "        Data : all sentences that are associated with its original comment\n",
    "        listSentence : list of sentences\n",
    "        listeReview : list of comments\n",
    "        listeIndex : list of index of the original comment\n",
    "        \n",
    "    Out : Data\n",
    "    \"\"\"\n",
    "    \n",
    "    data=pd.DataFrame()\n",
    "    listSentence=[]\n",
    "    listeReview=[]\n",
    "    listeIndex=[]\n",
    "    try:\n",
    "        df['index']=[i for i in range(len(df))]\n",
    "    except:\n",
    "        print(\"Index already exist\")\n",
    "    \n",
    "    for i in tqdm_notebook(range(len(df))):\n",
    "        try:\n",
    "            comments = detect_sentences(df[column_name][i])\n",
    "            for sent in range (len(comments)):\n",
    "                listSentence.append(comments[sent])\n",
    "                listeReview.append(df[column_name][i])\n",
    "                listeIndex.append(i)\n",
    "        except:\n",
    "            listSentence.append(\"\")\n",
    "            listeReview.append(\"\")\n",
    "            listeIndex.append(i)\n",
    "            \n",
    "    data[\"Index\"]=listeIndex \n",
    "    data[column_name]=listeReview\n",
    "    data[\"Sentence\"]=listSentence\n",
    "    \n",
    "    return Data\n",
    "\n",
    "\n",
    "def comments_into_sentences(name, column_name = 'REVIEW'):\n",
    "    \"\"\"main function to cut comments into sentences\n",
    "    \n",
    "    Parameters : \n",
    "        name : name of the spreadsheet with comments\n",
    "        column_name (DEFAULT = REVIEW) : name of the column where comments are located in the spreadsheet\n",
    "        \n",
    "    Attributes:\n",
    "        data : matrix with values imported from the spreadsheet\n",
    "        Data_Cut : matrix with sentences and associated comments of each sentence with originals index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data=pd.read_excel(\"../data/\"+name +\".xlsx\")\n",
    "    except:\n",
    "        print(\"file \" +name+ \" not found\")\n",
    "    data=pd.DataFrame(data)\n",
    "    data_cut=duplicate_row(data, column_name)\n",
    "    pd.DataFrame(data_cut).to_csv(\"../data/g4_\"+name+\"_sentences_v1.csv\")\n",
    "    print(\"g4_\"+name+\"_sentences_v1.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighting of the word embedding by tfidf\n",
    "\n",
    "def import_TFIDF(name):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    file (String ): Name of file \n",
    "\n",
    "    Attributes :\n",
    "    data : DataFrame of the tf-idf\n",
    "\n",
    "    out : data \n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(\"../data/g4_tfidf_\" + name + \".csv\") #import data of the tf-idf\n",
    "    except:\n",
    "        print(\"No tf-idf found in directory 'data' for the name : \" + name)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calcul_coordinates(words):\n",
    "    \"\"\"Documentation\n",
    "    \n",
    "    Parameters :\n",
    "    words (DataFrame) : words we want to calculate coords \n",
    "\n",
    "    Attributes:\n",
    "    list_coordinates (Array) : list of all coordinates of words \n",
    "    list_words(Array): list of all words\n",
    "    useless_words : list of words that the program is unable to transform into coordinates  \n",
    "\n",
    "    Out : \n",
    "    (final_list),useless_words : list of the matrix of coordinates from words and the list of useless words  \n",
    "    \"\"\"\n",
    "    list_coordinates=[]\n",
    "    list_words=[]\n",
    "    useless_words=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            list_coordinates.append((dico[words[i]]))\n",
    "            list_words.append(words[i])\n",
    "        except :\n",
    "            useless_words.append(words[i])\n",
    "    final_list=pd.concat([pd.DataFrame(list_words),pd.DataFrame(list_coordinates)],axis=1,ignore_index=True)\n",
    "    \n",
    "    return ((final_list),useless_words)\n",
    "\n",
    "\n",
    "def weight_embedding (name):\n",
    "    \"\"\"Function importing data from tf-idf and embedding and weighting values from the embedding with those from tf-idf\n",
    "    \n",
    "    Parameters:\n",
    "        name : name of the file that you want to weight\n",
    "        \n",
    "    Attributes:\n",
    "        tf_idf : matrix of the tf-idf with the input name\n",
    "        embedding : matrix of the embedding with the input name\n",
    "        words : list of words got from tf-idf matrix\n",
    "        coordiantes : list of coordianates of each word\n",
    "        useless_words : list of words that the program is unable to transform into coordinates\n",
    "        \n",
    "    Out :\n",
    "        Weighted matrix\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    tf_idf = import_TFIDF(name) \n",
    "    \n",
    "    words = tf_idf.columns.tolist()\n",
    "    words=words[2:] #deleting the 2 first words of the list which are labels \n",
    "    \n",
    "    coordinates, useless_words = calcul_coordinates(words)\n",
    "    \n",
    "    tf_idf=tf_idf.drop(useless_words,axis=1) # droping columns of the words that we are unable to calculate coordiantes with\n",
    "    #Commentaires=tf_idf[\"commentaire\"]\n",
    "    index=tf_idf[\"Unnamed: 0\"]\n",
    "    tf_idf=tf_idf.drop([\"Unnamed: 0\"],axis=1) # droping comments and index columns we won't need\n",
    "    coordinates=coordinates.set_index([0]) #setting words as index of the matrix\n",
    "\n",
    "    weighted_matrix = []\n",
    "    for i in range(len(tf_idf)):\n",
    "        weighted_matrix.append(coordinates.mul(tf_idf.loc[i, :].T, axis = 0).sum(axis = 0) / tf_idf.loc[i, :].sum())\n",
    "    embedingPonderate=pd.DataFrame(weighted_matrix)\n",
    "    embedingPonderate.insert(0,\"index\",index)\n",
    "   \n",
    "    embedingPonderate.to_csv(\"../data/g4_weight_embedding_\" + name + '.csv', sep =',')\n",
    "    \n",
    "    return embedingPonderate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_embedding(file):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    file (String ): Name of file \n",
    "\n",
    "    Attributes :\n",
    "    data (DataFrame) : Data \n",
    "    comments (DataFrame) : All clean comments \n",
    "\n",
    "    out : comments \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(\"../data/g4_tfidf_\" + file + \".csv\", index_col=0) # import data\n",
    "    try:\n",
    "        initial_file = pd.read_excel(\"../data/\" + file + \".xlsx\")\n",
    "    except:\n",
    "        initial_file = pd.read_csv(\"../data/\" + file + \".csv\")\n",
    "    data['index'] = data.index\n",
    "    data['commentaire'] = initial_file[\"Sentence\"].loc[data.index]\n",
    "    \n",
    "    comments = data.loc[:,lambda data: [\"index\", \"commentaire\"]]  #Recover all comments \n",
    "    return comments\n",
    "\n",
    "def coordinates(comments):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    comments (DataFrame) : All clean comments \n",
    "\n",
    "    Attributes:\n",
    "    coordinates (Array) : All coordinates of comments \n",
    "    coordinate(Array): coordinates of one comment \n",
    "    counter ( integer): Count number of words in a comments \n",
    "\n",
    "    Out : \n",
    "    coordinates (Array) : All coordinates of comments \n",
    "    \"\"\"\n",
    "\n",
    "    coordinates = np.array([[0]*201]) #Initializes array of coordonates \n",
    "    \n",
    "    for i in comments['index']:\n",
    "        coordinate = np.array([[0]*200]) #Create array of coordonates for one comment \n",
    "        counter = 0\n",
    "        try:\n",
    "            for word in comments['commentaire'][i].split(): #Iterate through the comments \n",
    "                try:\n",
    "                    coordinate = coordinate + np.array([dico[word]]) #Add coordinates for each word \n",
    "                    counter += 1\n",
    "                except:\n",
    "                    word\n",
    "            coordinate = np.concatenate((np.array([[i]]), coordinate/counter), axis=1) #Coordinates of comment \n",
    "            if (i==0):\n",
    "                coordinates = coordinate\n",
    "            else:\n",
    "                coordinates = np.concatenate((coordinates, coordinate), axis=0) # Add the coordonates of one comment to the array of coordinates\n",
    "        except:\n",
    "            i\n",
    "        print(str(i) + '/' + str(len(comments)), end=\"\\r\")  \n",
    "    return coordinates\n",
    "            \n",
    "def matrix(coordinates, comments):\n",
    "    \n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    comments (DataFrame) : All clean comments \n",
    "    coordinates (Array) : All coordinates of comments \n",
    "\n",
    "    Attributes:\n",
    "    coordinates (Array) : All coordinates of comments \n",
    "    liste(list):columns's Names \n",
    "\n",
    "    Out : \n",
    "    embedding ( Array): Coordinates + comments \n",
    "    \"\"\"\n",
    "    liste= ['index']\n",
    "    for i in range(1, 201):\n",
    "        liste.append(i) #Create index list \n",
    "    \n",
    "    coordinates = pd.DataFrame(coordinates, columns = liste) # Transfor array to dataFrame \n",
    "    embedding = comments.join(coordinates, on=['index'], how='inner', lsuffix='_caller', rsuffix='_other') #Join the comments to the coordinates \n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def embedding(file):\n",
    "    \"\"\"Documentation\n",
    "    \n",
    "    Parameters :\n",
    "    file (String ): Name of file \n",
    "    \n",
    "    Attributes:\n",
    "    comments (DataFrame) : All clean comments \n",
    "    coordinate (Array) : All coordinates of comments \n",
    "    embedding ( Array): Coordinates + comments \n",
    "    \n",
    "    Out : \n",
    "    Save the Dataframe Embedding  \n",
    "    \"\"\"\n",
    "    comments = database_embedding(file)\n",
    "    coordinate = coordinates(comments)\n",
    "    embedding = matrix(coordinate, comments).drop(columns='index_caller').drop(columns='index_other') #Drop useless columns \n",
    "    pd.DataFrame(embedding).set_index('index').to_csv(\"../data/g4_embedding_\" + file + \".csv\", sep = \",\") # save matrice\n",
    "    print('Matrice embedding de ' + file + ' enregistrée')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_purity(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function allows us to create a matrix to evaluate our methods \n",
    "\n",
    "    Parameters :\n",
    "    file (String) : A filename\n",
    "\n",
    "    Out : \n",
    "    Saves a file containing for each sentence the identified facets\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_excel('../data/'+file+'.xlsx', sep=',')\n",
    "    data = data[['LABEL_SEAT', 'LABEL_BED',\n",
    "                                       'LABEL_IFE', 'LABEL_LAVATORY SPACE',\n",
    "                                       'LABEL_TEMPERATURE', 'LABEL_HUMIDITY',\n",
    "                                       'LABEL_NOISE', 'LABEL_LOST BAGGAGE',\n",
    "                                       'LABEL_CHECK IN', 'LABEL_BOARDING',\n",
    "                                       'LABEL_SAV', 'LABEL_FOOD',\n",
    "                                       'LABEL_CABIN CREW', 'LABEL_PRICE',\n",
    "                                       'LABEL_PUNCTUALITY']]\n",
    "    \n",
    "    atmosphere = [0] * len(data)\n",
    "    baggage = [0] * len(data)\n",
    "    cabin_crew = [0] * len(data)\n",
    "    comfort = [0] * len(data)\n",
    "    empty = [0] * len(data)\n",
    "    food = [0] * len(data)\n",
    "    not_flight = [0] * len(data)\n",
    "    price = [0] * len(data)\n",
    "    punctuality = [0] * len(data)\n",
    "\n",
    "    for i in tqdm_notebook(range(len(data))):\n",
    "        if data['LABEL_SEAT'].iloc[i] > 0 or data['LABEL_BED'].iloc[i] > 0 or \\\n",
    "           data['LABEL_IFE'].iloc[i] > 0 or data['LABEL_LAVATORY SPACE'].iloc[i] > 0:\n",
    "            comfort[i] = 1\n",
    "        if data['LABEL_TEMPERATURE'].iloc[i] > 0 or data['LABEL_HUMIDITY'].iloc[i] > 0 or \\\n",
    "             data['LABEL_NOISE'].iloc[i] > 0:\n",
    "            atmosphere[i] = 1\n",
    "        if data['LABEL_LOST BAGGAGE'].iloc[i] > 0:\n",
    "            baggage[i] = 1\n",
    "        if data['LABEL_CHECK IN'].iloc[i] > 0 or data['LABEL_BOARDING'].iloc[i] > 0 or \\\n",
    "             data['LABEL_SAV'].iloc[i] > 0:\n",
    "            not_flight[i] = 1\n",
    "        if data['LABEL_FOOD'].iloc[i] > 0:\n",
    "            food[i] = 1\n",
    "        if data['LABEL_CABIN CREW'].iloc[i] > 0:\n",
    "            cabin_crew[i] = 1\n",
    "        if data['LABEL_PRICE'].iloc[i] > 0:\n",
    "            price[i] = 1\n",
    "        if data['LABEL_PUNCTUALITY'].iloc[i] > 0:\n",
    "            punctuality[i] = 1\n",
    "        if data.sum(axis = 1).iloc[i] == 0:\n",
    "            empty[i] = 1\n",
    "            \n",
    "    data_facets_group = pd.DataFrame()\n",
    "    data_facets_group['atmosphere'] = atmosphere\n",
    "    data_facets_group['baggage'] = baggage\n",
    "    data_facets_group['cabin_crew'] = cabin_crew\n",
    "    data_facets_group['comfort'] = comfort\n",
    "    data_facets_group['empty'] = empty\n",
    "    data_facets_group['food'] = food\n",
    "    data_facets_group['not_flight'] = not_flight\n",
    "    data_facets_group['price'] = price\n",
    "    data_facets_group['punctuality'] = punctuality\n",
    "    data_facets_group.index = data.index\n",
    "    \n",
    "    data_facets_group.to_csv(\"../data/\"+file+\"_new_facets.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
