{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on January 17\\nGroup 4\\nSub Group : Facet\\n@author : C.G  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on January 17\n",
    "Group 4\n",
    "Sub Group : Facet\n",
    "@author : C.G  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_facet(file, facet):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    The purpose of this function is to determine the important words for the facet entered in parameter\n",
    "    \n",
    "    Input\n",
    "        file (String) -> filename \n",
    "        facet (String) -> Name of the facet\n",
    "        \n",
    "    Output:\n",
    "        A score for supervised learning on facet\n",
    "        A graph showing the most important words for the facet \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tf_idf = pd.read_csv('../data/g4_tfidf_'+file+'.csv',\n",
    "                         sep=',', index_col=0)\n",
    "    all_data_annotate_facets_group = pd.read_csv(\"../data/\"+file+\"_new_facets.csv\", sep=',')\n",
    "    tf_idf = tf_idf.iloc[:, 1:]\n",
    "    all_data_annotate = all_data_annotate_facets_group.iloc[tf_idf.index]\n",
    "    y = all_data_annotate[facet]\n",
    "    y = list(y.fillna(0))\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=50, max_depth=50)\n",
    "    scores = cross_val_score(clf, tf_idf, y, cv=5)\n",
    "    print('Scores for facet ' + facet + ' : ', np.mean(scores))\n",
    "    \n",
    "    clf.fit(tf_idf, y)\n",
    "    feat_importances = pd.Series(clf.feature_importances_, index=tf_idf.columns)\n",
    "    feat_importances.nlargest(20).plot(kind='barh')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facet_for_sentence(words_predict, index, dict_facets, topic_predict, method_name, file):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function 'automatically' assigns one or more facets to the sentences.\n",
    "    \n",
    "    Input\n",
    "        words_predict (list) -> list of words for each cluster \n",
    "        dict_facets (dico) -> list of words for each facets\n",
    "        topic_predict (list) -> \n",
    "        method_name (String) -> Name of method use to predict our cluster\n",
    "        file (String) -> filename use\n",
    "        \n",
    "    Output:\n",
    "        facet prediction\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    facets_list = []\n",
    "    for i in range(len(words_predict)):\n",
    "        facet = []\n",
    "        for word in words_predict[i]:\n",
    "            if word in dict_facets['baggage_facet']:\n",
    "                facet.append('baggage')\n",
    "            if word in dict_facets['atmosphere_facet']:\n",
    "                facet.append('atmosphere')\n",
    "            if word in dict_facets['cabin_crew_facet']:\n",
    "                facet.append('cabin_crew')\n",
    "            if word in dict_facets['comfort_facet']:\n",
    "                facet.append('comfort')\n",
    "            if word in dict_facets['food_facet']:\n",
    "                facet.append('food')\n",
    "            if word in dict_facets['not_flight_facet']:\n",
    "                facet.append('not_flight')\n",
    "            if word in dict_facets['price_facet']:\n",
    "                facet.append('price')\n",
    "            if word in dict_facets['punctuality_facet']:\n",
    "                facet.append('punctuality')\n",
    "        if len(facet) > 0:\n",
    "            facets_list.append(facet)\n",
    "        else:\n",
    "            facets_list.append(['empty'])\n",
    "\n",
    "    df = pd.DataFrame([[0, 0, 0, 0, 0, 0, 0, 0, 0]] * len(topic_predict))\n",
    "    df.columns = [['atmosphere', 'baggage', 'cabin_crew', 'comfort',\n",
    "                   'empty', 'food', 'not_flight', 'price', 'punctuality']]\n",
    "    df.index = index\n",
    "\n",
    "\n",
    "    for i in tqdm_notebook(range(len(topic_predict))):\n",
    "        index = df.index[i]\n",
    "        cluster = topic_predict[i]\n",
    "        df.loc[index, [facets_list[cluster]]] = 1\n",
    "    \n",
    "    \n",
    "    df.to_csv('../data/g4_facet_'+method_name+'_prediction_on_'+file+'.csv', sep=',')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_tfidf(nb_cluster, file, dict_facets):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    TThis function allows you to cluster sentences with an LDA method.\n",
    "    \n",
    "    Input\n",
    "        nb_cluster (int) -> Number of clusters \n",
    "        file (String) -> filename use\n",
    "        dict_facets (dico) -> list of words for each facets        \n",
    "        \n",
    "    Output:\n",
    "        top words for eache cluster\n",
    "        a cluster prediction for each sentence\n",
    "        facet prediction\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tf_idf = pd.read_csv('../data/g4_tfidf_'+file+'.csv',\n",
    "                          sep=',', index_col=0)\n",
    "    tf_idf = tf_idf.iloc[:, 1:]\n",
    "    \n",
    "    columns = tf_idf.columns\n",
    "    \n",
    "    LDA = LatentDirichletAllocation(learning_method='batch', n_components=nb_cluster,\n",
    "                                    random_state=42)\n",
    "    LDA.fit(tf_idf)\n",
    "    \n",
    "    first_topic = LDA.components_[0]\n",
    "    top_topic_words = first_topic.argsort()[-10:]\n",
    "    \n",
    "    cluster_number = []\n",
    "    top_words_topic =  []\n",
    "    for i, topic in enumerate(LDA.components_): \n",
    "        cluster_number.append(i)\n",
    "        top_words_topic.append([list(columns[2:])[i]\n",
    "                                for i in topic.argsort()[-15:]])\n",
    "\n",
    "    top_words = pd.DataFrame(top_words_topic)\n",
    "    top_words.to_csv('../data/LDA_top_word_for_'+str(len(cluster_number))+'_cluster.csv')\n",
    "    \n",
    "    topic_values = LDA.transform(tf_idf)\n",
    "    topic_predict = topic_values.argmax(axis=1)\n",
    "    \n",
    "    cluster_prediction = pd.DataFrame(topic_predict, index = tf_idf.index, columns = ['cluster'])\n",
    "    \n",
    "    cluster_prediction.to_csv('../data/LDA_cluster_sentence_'+file+'.csv')\n",
    "    \n",
    "    \n",
    "    facets_predictions = facet_for_sentence(top_words_topic, tf_idf.index, dict_facets, topic_predict, 'LDA', file)\n",
    "    \n",
    "    return top_words, cluster_prediction, facets_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_doc_per_cluster(cluster: np.array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function give the n_terms most important word from each cluster\n",
    "\n",
    "    Parameters:\n",
    "        cluster: array from model.labels_\n",
    "\n",
    "    Out :\n",
    "        print the nb of document in each cluster\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nbCluster = len(np.unique(cluster))\n",
    "    somme = 0\n",
    "    for num_cluster in range(nbCluster):\n",
    "        nb_element = list(cluster).count(num_cluster)\n",
    "        print('Cluster ' + str(num_cluster) + ' has : ' +\n",
    "              str(nb_element) + ' documents')\n",
    "        somme += nb_element\n",
    "\n",
    "    print('Total : ' + str(len(cluster)) +\n",
    "          ' element on ' + str(somme))\n",
    "    \n",
    "    \n",
    "def one_word_per_cluster(clusters: MiniBatchKMeans, matrix_tfidf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function return a DataFrame which represent the weight of the words in each cluster.\n",
    "        A word can only be in an unique cluster.\n",
    "\n",
    "    Parameters:\n",
    "        clusters: element given by a clustering fonction (here MiniBatchKMeans)\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "\n",
    "    Out :\n",
    "        cluster_center: a DataFrame cluster X Word with one score per word (a word is in an unique cluster)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_center = pd.DataFrame(clusters.cluster_centers_)\n",
    "    cluster_center.columns = matrix_tfidf.columns\n",
    "\n",
    "    # We run cluster_center (for each word (column), we check all clusters (row))\n",
    "    for i in cluster_center.columns:\n",
    "\n",
    "        # If the presence of a word is not in the majority in a cluster compared to the others -> Delete column\n",
    "        if (np.array(cluster_center[i].sort_values(ascending=False))[0] >\n",
    "                2 * np.array(cluster_center[i].sort_values(ascending=False))[1]) == False:\n",
    "            cluster_center = cluster_center.drop(columns=i)\n",
    "\n",
    "        # Else we put 0 to the weights for the clusters out of the majority\n",
    "        else:\n",
    "            maxi = cluster_center[i].max()\n",
    "            cluster_center.loc[cluster_center[i] != maxi, i] = 0\n",
    "\n",
    "    return cluster_center\n",
    "\n",
    "\n",
    "def get_keyword(cluster_center_unique: pd.DataFrame, nb_keyword: int) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function give the most nb_keyword representative words of each cluster\n",
    "\n",
    "    Parameters:\n",
    "        cluster_center_unique: a DataFrame cluster X Word with one score per word\n",
    "        nb_keyword: number of keywords wanted per cluster\n",
    "\n",
    "    Out :\n",
    "        list_key_word: list of the list of nb_keyword for each cluster\n",
    "\n",
    "\n",
    "    N.B. :\n",
    "        We could calculate the weight of each word and only keep the words with the most important weight.\n",
    "        Not simply the n most importante (some cluster could have only one or two important word)\n",
    "    \"\"\"\n",
    "\n",
    "    # We run the new tf-idf (1 value per column <=> a word is only in one cluster)\n",
    "    list_key_word = []\n",
    "    for i, r in cluster_center_unique.iterrows():\n",
    "        # We collect the most present nb_words and their weights\n",
    "        keywords = [cluster_center_unique.columns[t]\n",
    "                    for t in np.argsort(r)[-nb_keyword:]]\n",
    "        \n",
    "        #keywords_weight = [r.sort_values(ascending=False)[keywords[t]] for t in range(nb_word)]\n",
    "        keywords.reverse()\n",
    "        # keywords_weight.reverse()\n",
    "        list_key_word.append(keywords)\n",
    "\n",
    "    return list_key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attrib_facet_to_cluster(keyword_per_cluster: pd.DataFrame,atmosphere_facet,\n",
    "                            baggage_facet, cabin_crew_facet, comfort_facet,\n",
    "                            food_facet, not_flight_facet, price_facet,punctuality_facet\n",
    "                            ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function attrib the facets to the clusters.\n",
    "        It create a link between the keyword list and the facets_lists.\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster: number of cluster to test\n",
    "        atmosphere_facet: list of the words referenced a facet\n",
    "\n",
    "    Out :\n",
    "        pd.DataFrame cluster X facette (on column)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    facets_list = []\n",
    "\n",
    "    for i in keyword_per_cluster.index:\n",
    "        facette = []\n",
    "        for word in keyword_per_cluster.loc[i]:\n",
    "            if word in baggage_facet:\n",
    "                facette.append('BAGGAGE')\n",
    "            if word in atmosphere_facet:\n",
    "                facette.append('ATMOSPHERE')\n",
    "            if word in cabin_crew_facet:\n",
    "                facette.append('CABIN_CREW')\n",
    "            if word in comfort_facet:\n",
    "                facette.append('COMFORT')\n",
    "            if word in food_facet:\n",
    "                facette.append('FOOD')\n",
    "            if word in not_flight_facet:\n",
    "                facette.append('NOT_FLIGHT')\n",
    "            if word in price_facet:\n",
    "                facette.append('PRICE')\n",
    "            if word in punctuality_facet:\n",
    "                facette.append('PUNCTUALITY')\n",
    "\n",
    "        if len(facette) > 0:\n",
    "            facets_list.append(facette)\n",
    "        else:\n",
    "            facets_list.append(['EMPTY'])\n",
    "\n",
    "    # facet duplicate suppression\n",
    "    for i in range(len(facets_list)):\n",
    "        liste = list(set(facets_list[i]))\n",
    "        facets_list[i] = liste\n",
    "\n",
    "    return pd.DataFrame(facets_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def attrib_facet_to_doc(pd_rep_facet_to_cluster: pd.DataFrame, list_facets: list,\n",
    "                        index: list, rep_cluster: np.array) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function attrib the facets to the documents.\n",
    "        It create a link between the DataFrame return by attrib_facet_to_cluster\n",
    "        and the repartition of the document into the clusters return by cluster.labels_\n",
    "\n",
    "    Parameters:\n",
    "        pd_rep_facet_to_cluster:\n",
    "        list_facets:\n",
    "        index:\n",
    "        rep_cluster:\n",
    "\n",
    "    Out :\n",
    "        pd.DataFrame doc X facette (on column)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rep_doc_to_facet = pd.DataFrame(0, index=index, columns=list_facets)\n",
    "    for cluster_doc in rep_cluster :\n",
    "        row = pd_rep_facet_to_cluster.loc[cluster_doc]\n",
    "        for facet in row:\n",
    "            if facet is not None:\n",
    "                rep_doc_to_facet.loc[cluster_doc][facet] = 1\n",
    "                \n",
    "    return rep_doc_to_facet\n",
    "\n",
    "\n",
    "\n",
    "def MSE(nb_cluster_max: int, matrix_tfidf: pd.DataFrame) -> print:\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the MSE Criterion to estimate the number of cluster\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster_max: number of cluster to test\n",
    "\n",
    "    Out :\n",
    "        plot of the MSE result\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_components = np.arange(1, nb_cluster_max)\n",
    "    models = [MiniBatchKMeans(n_clusters=n, random_state=42).fit(\n",
    "        matrix_tfidf) for n in n_components]\n",
    "    plt.plot(n_components, [m.inertia_ for m in models], label='MSE')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('n_components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(df, col_cluster, col_label):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    df (DataFrame) : Matrix of coordinates of comments\n",
    "    clo_cluster (String) : Name of column of clusters\n",
    "    col_label (String) : Name of column of index\n",
    "\n",
    "    Attributes :\n",
    "    moy (Int) : Mean\n",
    "    nb_cluster (Int) : Number of clusters\n",
    "\n",
    "    Out : moy\n",
    "    \"\"\"\n",
    "    moy = 0\n",
    "    nb_cluster = len(df[col_cluster].unique())\n",
    "    for i in np.unique(df[col_cluster]):\n",
    "        # recover comments of each cluster\n",
    "        rows_in_cluster = df[df[col_cluster] == i][col_label]\n",
    "        moy += rows_in_cluster.sum(axis=0).nlargest(n=1).sum() / \\\n",
    "            rows_in_cluster.sum().sum()\n",
    "    return moy/nb_cluster\n",
    "\n",
    "\n",
    "def purity_LDA(matrix_tfidf: pd.DataFrame, nb_clusters: int) -> print:\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the purity score of clusters for different number of cluster and\n",
    "        plot the result.\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster_max: number of cluster to test\n",
    "\n",
    "    Out :\n",
    "        The different result of purity for the different number of clusters\n",
    "    \"\"\"\n",
    "\n",
    "    all_data_annotate_facets_group = pd.read_csv(\n",
    "        \"../data/ALL_DATA_ANNOTATE_new_facets.csv\", sep=',')\n",
    "    metric_all_data_annotate_facets_group = all_data_annotate_facets_group.iloc[:, 1:]\n",
    "\n",
    "    LDA = LatentDirichletAllocation(learning_method='batch', n_components=nb_clusters,\n",
    "                                    random_state=42)\n",
    "    LDA.fit(matrix_tfidf)\n",
    "    topic_values = LDA.transform(matrix_tfidf)\n",
    "    topic_predict = topic_values.argmax(axis=1)\n",
    "\n",
    "    df = pd.DataFrame(topic_predict, index=matrix_tfidf.index,\n",
    "                      columns=['cluster'])\n",
    "\n",
    "    metric_all_data_annotate_facets_group2 = metric_all_data_annotate_facets_group.loc[\n",
    "        df.index]\n",
    "    metric_all_data_annotate_facets_group2['cluster'] = df[\"cluster\"]\n",
    "\n",
    "    return purity(metric_all_data_annotate_facets_group2, 'cluster',\n",
    "                  ['atmosphere', 'baggage', 'cabin_crew', 'comfort', 'empty', 'food', 'not_flight', 'price', 'punctuality'])\n",
    "\n",
    "\n",
    "def purity_Kmeans_pp(matrix_tfidf: pd.DataFrame, nb_clusters: int) -> print:\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the purity score of clusters for different number of cluster and\n",
    "        plot the result.\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster_max: number of cluster to test\n",
    "        index: list of index\n",
    "\n",
    "    Out :\n",
    "        The different result of purity for the different number of clusters\n",
    "    \"\"\"\n",
    "\n",
    "    all_data_annotate_facets_group = pd.read_csv(\n",
    "        \"../data/ALL_DATA_ANNOTATE_new_facets.csv\", sep=',')\n",
    "    metric_all_data_annotate_facets_group = all_data_annotate_facets_group.iloc[:, 1:]\n",
    "\n",
    "    clusters = MiniBatchKMeans(\n",
    "        n_clusters=nb_clusters, random_state=42).fit(matrix_tfidf)\n",
    "\n",
    "    df = pd.DataFrame(clusters.labels_,\n",
    "                      index=matrix_tfidf.index, columns=['cluster'])\n",
    "\n",
    "    metric_all_data_annotate_facets_group2 = metric_all_data_annotate_facets_group.loc[\n",
    "        df.index]\n",
    "    metric_all_data_annotate_facets_group2['cluster'] = df[\"cluster\"]\n",
    "\n",
    "    return purity(metric_all_data_annotate_facets_group2, 'cluster',\n",
    "                  ['atmosphere', 'baggage', 'cabin_crew', 'comfort', 'empty', 'food', 'not_flight', 'price', 'punctuality'])\n",
    "\n",
    "def purity_Kmeans(data, nb_cluster, index):\n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the purity score of clusters for different number of cluster and\n",
    "        plot the result.\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster_max: number of cluster to test\n",
    "        index: list of index\n",
    "    Out :\n",
    "        The different result of purity for the different number of clusters\n",
    "    \"\"\"\n",
    "    all_data_annotate_facets_group = pd.read_csv(\n",
    "        \"../data/ALL_DATA_ANNOTATE_new_facets.csv\", sep=',')\n",
    "    metric_all_data_annotate_facets_group = all_data_annotate_facets_group.iloc[:, 1:]\n",
    "    metric_all_data_annotate_facets_group2 = metric_all_data_annotate_facets_group.loc[index]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=nb_cluster, random_state=42,\n",
    "                    n_init=30).fit(data)  # create clusters\n",
    "    rep_cluster = kmeans.labels_  # list of clusters of comments\n",
    "    metric_all_data_annotate_facets_group2['cluster'] = rep_cluster\n",
    "\n",
    "    return purity(metric_all_data_annotate_facets_group2,\n",
    "                  'cluster', ['atmosphere', 'baggage', 'cabin_crew', 'comfort', 'empty', 'food', 'not_flight', 'price', 'punctuality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silouhette_LDA(nb_clusters: int, matrix_tfidf: pd.DataFrame) -> print:\n",
    "   \n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the silouhette\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster : number of cluster to test\n",
    "\n",
    "    Out :\n",
    "        silhouette result\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    LDA = LatentDirichletAllocation(learning_method='batch', n_components=nb_clusters,\n",
    "                                    random_state=42)\n",
    "    LDA.fit(matrix_tfidf)\n",
    "    topic_values = LDA.transform(matrix_tfidf)\n",
    "    topic_predict = topic_values.argmax(axis=1)\n",
    "    \n",
    "    return metrics.silhouette_score(matrix_tfidf, topic_predict)\n",
    "\n",
    "\n",
    "def silouhette_Kmeans_pp(nb_clusters: int, matrix_tfidf: pd.DataFrame) -> print:\n",
    "   \n",
    "    \"\"\"\n",
    "    Documentation:\n",
    "        This function calculate the silouhette\n",
    "\n",
    "    Parameters:\n",
    "        matrix_tfidf: a tf_idf DataFrame Doc X Word\n",
    "        nb_cluster : number of cluster to test\n",
    "\n",
    "    Out :\n",
    "        silhouette result\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    clusters = MiniBatchKMeans(n_clusters=nb_clusters, random_state=42).fit(matrix_tfidf)\n",
    "    return metrics.silhouette_score(matrix_tfidf, clusters.labels_)\n",
    "\n",
    "\n",
    "\n",
    "def silhouette_Kmeans(nb_clusters: int, matrix: pd.DataFrame) -> print:\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    nb_cluster (Int) : Numbers of clusters\n",
    "    matrix (DataFrame) : Matrix of coordinates of comments\n",
    "\n",
    "    Attributes :\n",
    "    res (list) : list of clusters\n",
    "    clusters (clusters) : \n",
    "    \"\"\"\n",
    "    clusters = KMeans(n_clusters=nb_clusters, random_state=42,\n",
    "                      n_init=30).fit(matrix)  # create clusters\n",
    "    return metrics.silhouette_score(matrix, clusters.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_read(file, weighted=False):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    file (String): Name of file \n",
    "\n",
    "    Attributes :\n",
    "    data (DataFrame) : Data\n",
    "    index (DataFrame) : Index\n",
    "    comments (DataFrame) : All clean comments\n",
    "\n",
    "    Out : data, index, comments \n",
    "    \"\"\"\n",
    "    if not weighted:\n",
    "        data = pd.read_csv(\"../data/g4_embedding_\" +\n",
    "                           file + \".csv\")  # import data\n",
    "        data = data.dropna()  # drop empty comments\n",
    "        index = np.array(data['index'].astype(int))  # recover index\n",
    "        comments = np.array(data['commentaire'])  # recover comments\n",
    "        del data['index']  # delete index column\n",
    "        del data['commentaire']  # delete comments column\n",
    "    else:\n",
    "        data = pd.read_csv(\"../data/g4_weight_embedding_\" +\n",
    "                           file + \".csv\", index_col=1)  # import data\n",
    "        data_for_comments = pd.read_csv(\n",
    "            \"../data/g4_embedding_\" + file + \".csv\")\n",
    "        data = data.dropna()  # drop empty comments\n",
    "        index = data.index  # recover index\n",
    "        data = data.iloc[:, 1:]\n",
    "        # recover comments\n",
    "        comments = np.array(data_for_comments['commentaire'].loc[data.index])\n",
    "\n",
    "    return data, index, comments\n",
    "\n",
    "\n",
    "\n",
    "def cluster_embedding(data, index, comments, nb_clusters):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters :\n",
    "    data (DataFrame) : Data\n",
    "    index (DataFrame) : Index\n",
    "    comments (DataFrame) : All clean comments \n",
    "\n",
    "    Attributes :\n",
    "    kmeans \n",
    "    rep_cluster\n",
    "    distances (Array) : List of distances\n",
    "    coor_cluster (Array) : Gravity center of cluster\n",
    "    coor_comm (Array) : Coordinates of comment\n",
    "    dist_eucli (Int) : Distance between coor_cluster and coor_comm\n",
    "    matrix (DataFrame) : Table with index, comments, number of clusters and distances\n",
    "    clust(i) (dataFrame) : 5 more closed comments to the cluster i\n",
    "\n",
    "    Out : matrix\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans = KMeans(n_clusters=nb_clusters, random_state=42,\n",
    "                    n_init=20).fit(data)  # create clusters\n",
    "\n",
    "    rep_cluster = kmeans.labels_  # list of clusters of comments\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            coor_cluster = kmeans.cluster_centers_[\n",
    "                rep_cluster[i]]  # gravity center of cluster\n",
    "            coor_comm = np.array(data.iloc[i])\n",
    "            try:\n",
    "                # distance between gravity center of cluster and comment\n",
    "                dist_eucli = distance.euclidean(coor_cluster, coor_comm)\n",
    "            except:\n",
    "                print(i)\n",
    "        except:\n",
    "            print(i)\n",
    "        distances.append(dist_eucli)  # append this distance\n",
    "\n",
    "    # concatenate comments and distances\n",
    "    matrix = np.concatenate(\n",
    "        ([index], [comments], [rep_cluster], [distances]), axis=0)\n",
    "\n",
    "    matrix = pd.DataFrame(matrix).T\n",
    "    matrix.columns = ['index', 'commentaire',\n",
    "                      'cluster', 'distance']  # add columns names\n",
    "\n",
    "    v = locals()\n",
    "    words = []\n",
    "\n",
    "    for i in range(nb_clusters):\n",
    "\n",
    "        # all comments of cluster i\n",
    "        v['clust%d' % i] = matrix[matrix['cluster'] == i]\n",
    "\n",
    "        # 5 more closed to center gravity of cluster i\n",
    "        v['clust%d' % i] = v['clust%d' % i].sort_values(by='distance')[0:10]\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=0.1)\n",
    "        commentary = v['clust%d' % i]['commentaire']\n",
    "        commentary = commentary.dropna()\n",
    "        X = vectorizer.fit_transform(commentary)  # creating tf-idf matrix\n",
    "\n",
    "        # transforming in data frame\n",
    "        M = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "        words.append(M.sum().sort_values(ascending=False)[0:10].index)\n",
    "\n",
    "        # M.to_excel(\"../data/g4_tfidf_clust\" + str(i) + \".xlsx\") # save matrix\n",
    "\n",
    "    pd.DataFrame(words).to_excel(\"../data/g4_words_clusters.xlsx\")\n",
    "\n",
    "    return matrix, words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels_facet(tf_idf,facet,metric):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "    this function adds the label in the dataframe tf_idf \n",
    "    by putting 1 if the document talks about the label and 0 if not. \n",
    "    \n",
    "    Parameters:\n",
    "        tf_idf: dataframe tf_idf\n",
    "        facet: one facet\n",
    "        metric : grouping facet metric\n",
    "    \n",
    "    Output:\n",
    "        result: features et labels\n",
    "    \"\"\"   \n",
    "    \n",
    "    tf_idf[facet]=0\n",
    "    \n",
    "    for ind in metric.index:\n",
    "        if metric.loc[ind,[facet]][0]==1:\n",
    "            tf_idf.loc[ind,[facet]]=1\n",
    "      \n",
    "    tf_idf=tf_idf.fillna(0).astype(int)\n",
    "    tf_idf=tf_idf.sort_index(axis=0)\n",
    "       \n",
    "    labels=np.array(tf_idf[facet])\n",
    "    tf_idf=tf_idf.drop(facet,axis=1)\n",
    "    features= np.array (tf_idf)\n",
    "  \n",
    "    return features,labels\n",
    "\n",
    "def facet_score(tf_idf,facet,metric):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "    this function calculates the prediction score of a facet using the random forest classifier\n",
    " \n",
    "    \n",
    "    Parameters:\n",
    "        tf_idf: dataframe tf_idf\n",
    "        facet: one facet\n",
    "        metric : grouping facet and metric\n",
    "    \n",
    "    Output:\n",
    "        result: predictive facet score\n",
    "    \"\"\"     \n",
    "    \n",
    "    features,labels=features_labels_facet(tf_idf,facet,metric)\n",
    "    \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split (features, labels, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "    rf = RandomForestClassifier (n_estimators = 100, random_state = 42)\n",
    "    \n",
    "    rf.fit (train_features, train_labels)\n",
    "        \n",
    "    return rf.score(features,labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_cluster(tf_idf, n_clusters,n_terms):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function associates n_clusters clusters , \n",
    "        their n_terms comments of strong weight with the kmeans classification  \n",
    "        \n",
    "    Parameters:\n",
    "        tf_idf: dataframe tf_idf\n",
    "        n_clusters: the number of clusters.\n",
    "        n_terms : the n terms with the highest cluster weight\n",
    "    \n",
    "    Output:\n",
    "        result: dictionary of the n_clusters clusters with each one its n_terms strongest comments\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters = MiniBatchKMeans(n_clusters=n_clusters, random_state=42).fit_predict(tf_idf)\n",
    "    df = pd.DataFrame(tf_idf.to_dense()).groupby(clusters).mean()\n",
    "    dict_cluster={}\n",
    "    dict_max={}\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        dict_max[i+1]=[]\n",
    "        dict_cluster[i+1]=[]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for ind in df.index:\n",
    "            if df.loc[ind,[col]][0]==max(df[col]):\n",
    "                dict_max[ind+1].append(col)\n",
    "                break\n",
    "        \n",
    "    for i, r in df.iterrows():\n",
    "        n=1\n",
    "        len_dm=len(dict_max[i+1])\n",
    "        m=n_terms\n",
    "        if len_dm<n_terms:\n",
    "            m=len_dm\n",
    "        while len(dict_cluster[i+1])<m:\n",
    "            t=np.argsort(r)[-n]\n",
    "            if tf_idf.columns[t] in dict_max[i+1]:\n",
    "                dict_cluster[i+1].append(tf_idf.columns[t])\n",
    "            n+=1  \n",
    "            \n",
    "    return dict_cluster\n",
    "\n",
    "def transform_list_doc (comments,index_document):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function turns a document into a list of comments\n",
    "    \n",
    "    Parameters:\n",
    "        commentary: commentary from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        index_doc: index of commentary\n",
    "\n",
    "    \n",
    "    Output:\n",
    "        result: \n",
    "    \"\"\" \n",
    "    comment=comments.loc[index_document]\n",
    "    comment=str(comment)\n",
    "    comment=comment.replace(\",\",\"\")\n",
    "    comment=comment.split()\n",
    "    comment=comment[:-4][1:]\n",
    "    \n",
    "    return comment\n",
    "\n",
    "def cluster_belong_document(dict_cluster,comments):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function associates each cluster with a document list.    \n",
    "   \n",
    "   Parameters:\n",
    "        dict_cluster: dictionary of cluster\n",
    "        comments: commentary from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        \n",
    "    Output:\n",
    "        result: index of commentary and the clusters where they belong\n",
    "        \n",
    "    \"\"\"\n",
    "    dict_cluster_doc={}\n",
    "    for i in dict_cluster.keys():\n",
    "        dict_cluster_doc[i]=[]\n",
    "        \n",
    "    \n",
    "    for ind in comments.index:\n",
    "        comment=transform_list_doc(comments,ind)\n",
    "        for key, val in dict_cluster.items():\n",
    "            for com in comment:\n",
    "                if com in dict_cluster[key]:\n",
    "                    dict_cluster_doc[key].append(ind)\n",
    "                    break\n",
    "\n",
    "    \n",
    "    return dict_cluster_doc\n",
    "\n",
    "def comment_surround_facet(facet,comments):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function returns the list of comments surrounding a given facet\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "        facet: one facet\n",
    "        comments: comments from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        \n",
    "    Output:\n",
    "        result: list of comments surrounding a facet\n",
    "        \n",
    "    \"\"\"\n",
    "    list_comment=[]\n",
    "    for i in comments.index:\n",
    "        comment=transform_list_doc(comments,i)\n",
    "        n=len(comment)\n",
    "        for i in range(n):      \n",
    "            if comment[i] ==facet.lower() :\n",
    "                if i>0:\n",
    "                    list_comment.append(comment[i-1])\n",
    "                if i<n-1:\n",
    "                    list_comment.append(comment[i+1])\n",
    "            else :\n",
    "                for fac in facet.split():\n",
    "                    if comment[i] ==fac :\n",
    "                        if i>0:\n",
    "                            list_comment.append(comment[i-1])\n",
    "                        if i<n-1:\n",
    "                            list_comment.append(comment[i+1])\n",
    "    return list_comment\n",
    "\n",
    "\n",
    "def comment_frequent_facet(facet,comments):\n",
    "    \"\"\" \n",
    "    Documentation :\n",
    "        this function returns a list of the most frequent comments among the comments surrounding the facet\n",
    "    \n",
    "    Parameters:\n",
    "        facet: one facet\n",
    "        comments: comments from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        \n",
    "    Output:\n",
    "        result: list of the most frequent comments surrounding a facet\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    list_facet=comment_surround_facet(facet.lower(),comments)\n",
    "    comment_count=[]\n",
    "            \n",
    "    dict_facet={}\n",
    "    for s in list_facet:\n",
    "        if s not in comment_count:\n",
    "            dict_facet[s]=list_facet.count(s)\n",
    "            comment_count.append(s)\n",
    "    \n",
    "    list_comment_frequent=[c for c,v in dict_facet.items() if v==max(dict_facet.values())]\n",
    "\n",
    "    return list_comment_frequent\n",
    "\n",
    "\n",
    "def cluster_belong_facet(dict_cluster,comments,facets,dict_facets_ref):\n",
    "    \n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function associates each cluster with a facet list    \n",
    "    \n",
    "    Parameters:\n",
    "        dict_cluster: dictionary of cluster\n",
    "        comments: comments from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        facets : list of facet\n",
    "        dict_facets_ref: comments referring to the facet\n",
    "        \n",
    "    Output:\n",
    "        result: dictionary whose keys are clusters and values the facets\n",
    "        \n",
    "    \"\"\"\n",
    "    dict_cluster_facet={}\n",
    "    for i in dict_cluster.keys():\n",
    "        dict_cluster_facet[i]=[]\n",
    "        \n",
    "        \n",
    "    for facet in facets:\n",
    "        list_comment_freq_facet=comment_frequent_facet(facet,comments)\n",
    "        list_ref_facet=dict_facets_ref[facet]\n",
    "        for key, value in dict_cluster.items():\n",
    "                for val in value:\n",
    "                    if facet.lower()==val:\n",
    "                        dict_cluster_facet[key].append(facet)\n",
    "                        break\n",
    "                    elif val in list_comment_freq_facet:\n",
    "                        dict_cluster_facet[key].append(facet)\n",
    "                        break\n",
    "                    elif val in list_ref_facet:\n",
    "                        dict_cluster_facet[key].append(facet)\n",
    "                        break\n",
    "\n",
    "    return dict_cluster_facet\n",
    "\n",
    "def df_doc_facet(facets,comments,dict_facets_ref,tf_idf,dict_cluster):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function creates the document and facet dataframe. \n",
    "        For each cluster, attribute 1 the facets and documents contained in it.\n",
    "        \n",
    "    Parameters:\n",
    "        facets: facets\n",
    "        comments: comments from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        dict_facets_ref: comments referring to the facet\n",
    "        tf_idf : dataframe tf_idf\n",
    "        \n",
    "    Output:\n",
    "        result: dataframe document and facet\n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    dict_cluster_facet=cluster_belong_facet(dict_cluster,comments,facets,dict_facets_ref)\n",
    "    dict_cluster_doc=cluster_belong_document(dict_cluster,comments)\n",
    "    \n",
    "    \n",
    "    dim = (tf_idf.shape[0], len(facets))\n",
    "    df_facet = np.zeros(dim, dtype='int')\n",
    "    df_facet = pd.DataFrame(df_facet, index=tf_idf.index, columns=facets)\n",
    "    \n",
    "    for key_facet,val_facet in dict_cluster_facet.items():\n",
    "        for key_doc, val_doc in dict_cluster_doc.items():\n",
    "            if key_facet==key_doc:\n",
    "                for vf in val_facet:\n",
    "                    for vd in val_doc:\n",
    "                        df_facet.loc[vd,[vf]]=1\n",
    "\n",
    "                    \n",
    "    df_facet=df_facet.rename(columns={'CABIN CREW':'CABIN_CREW','LAVATORY SPACE':'LAVATORY_SPACE','LOST BAGGAGE':'LOST_BAGGAGE','CHECK IN':'CHECK_IN','ATTRACTIVE':'ATTRACTIVE_AIRCRAFT'})\n",
    "    df_facet.columns=[c.lower() for c in df_facet.columns]\n",
    "    return df_facet\n",
    "\n",
    "\n",
    "def df_doc_new_facet(facets,comments,dict_facets_ref,tf_idf,dict_cluster):\n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function creates the document and news facets dataframe. \n",
    "        For each cluster, attribute 1 the facets and documents contained in it.\n",
    "        \n",
    "    Parameters:\n",
    "        facets: facets\n",
    "        comments: comments from 'OTHER_DATA_ANNOTATE.xlsx'\n",
    "        dict_facets_ref: comments referring to the facet\n",
    "        tf_idf : dataframe tf_idf\n",
    "        \n",
    "    Output:\n",
    "        result: dataframe document and facet\n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    dict_cluster_facet=cluster_belong_facet(dict_cluster,comments,facets,dict_facets_ref)\n",
    "    dict_cluster_doc=cluster_belong_document(dict_cluster,comments)\n",
    "    \n",
    "    \n",
    "    dim = (tf_idf.shape[0], len(facets))\n",
    "    df_facet = np.zeros(dim, dtype='int')\n",
    "    df_facet = pd.DataFrame(df_facet, index=tf_idf.index, columns=facets)\n",
    "    \n",
    "    df_facet['EMPTY']=1\n",
    "    \n",
    "    for key_facet,val_facet in dict_cluster_facet.items():\n",
    "        for key_doc, val_doc in dict_cluster_doc.items():\n",
    "            if key_facet==key_doc:\n",
    "                for vf in val_facet:\n",
    "                    for vd in val_doc:\n",
    "                        df_facet.loc[vd,[vf]]=1\n",
    "                        df_facet.loc[vd,['EMPTY']]=0\n",
    "\n",
    "                    \n",
    "    df_facet=df_facet.rename(columns={'CABIN CREW':'CABIN_CREW'})\n",
    "    df_facet.columns=[c.lower() for c in sorted(df_facet.columns)]\n",
    "    \n",
    "    \n",
    "    return df_facet\n",
    "\n",
    "\n",
    "def create_dict_new_facet(facets):\n",
    "    \n",
    "    \"\"\" \n",
    "    Documentation:\n",
    "        this function creates a dictionnary by regrouping old facets\n",
    "        \n",
    "    Parameters:\n",
    "        facets: facets\n",
    "        \n",
    "    Output:\n",
    "        result: dictionnary \n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    \n",
    "    # The facets\n",
    "    facets = ['SEAT','BED','IFE','FOOD','NOISE','TEMPERATURE','HUMIDITY','CABIN CREW','LAVATORY SPACE','PRICE',\n",
    "              'LOST BAGGAGE','CHECK IN','PUNCTUALITY','ATTRACTIVE','SAV','BOARDING','GENERAL']\n",
    "\n",
    "    #comments referring to the facet\n",
    "    dict_facets_ref={\n",
    "        facets[0]:['foot','leg','legroom','room','window','space','seating','place','spacious'],\n",
    "        facets[1]:['sleep','flat'],\n",
    "        facets[2]:['screen','entertainment','movie'],\n",
    "        facets[3]:['drink','meal','vegetarian','water','tray','table'],\n",
    "        facets[4]:['bad','terrible','horrible'],\n",
    "        facets[5]:['cold','heat','warm'],\n",
    "        facets[6]:['moisture','damp','wet'],\n",
    "        facets[7]:['crew','cabin','english','spanish','entertainment','class',],\n",
    "        facets[8]:['aisle','toilet'],\n",
    "        facets[9]:['poor','budget','pay','value','fee','expensive'],\n",
    "        facets[10]:['lost','missing','bag','luggage','baggage'],\n",
    "        facets[11]:['check','log','register'],\n",
    "        facets[12]:['time','hour','minute','year','delay'],\n",
    "        facets[13]:['good','great','wonderful','excellent','efficient','need','comfort','comfortable','fine','cool','sweet','quality','amazing','nice'],\n",
    "        facets[14]:['service','information','communication'],\n",
    "        facets[15]:['terminal','board','departure','departure','leave','gate'],\n",
    "        facets[16]:['member','professional','airliner','chair','customer','trip','agent','company','travel','passenger','fly','aircraft','airplane','airport','plane','staff']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    dict_facets_new_ref={}\n",
    "    for i in range(len(facets_new)):\n",
    "        dict_facets_new_ref[facets_new[i]]=[]\n",
    "\n",
    "    for i in [0,1,2,8]:\n",
    "        for fac in dict_facets_ref[facets[i]]:\n",
    "            dict_facets_new_ref[facets_new[0]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[10]]:\n",
    "        dict_facets_new_ref[facets_new[1]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[7]]:\n",
    "        dict_facets_new_ref[facets_new[2]].append(fac)\n",
    "\n",
    "    for i in [4,5,6]:\n",
    "        for fac in dict_facets_ref[facets[i]]:\n",
    "            dict_facets_new_ref[facets_new[3]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[16]]:\n",
    "            dict_facets_new_ref[facets_new[4]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[3]]:\n",
    "        dict_facets_new_ref[facets_new[5]].append(fac)\n",
    "\n",
    "    for i in [11,15,14]:\n",
    "        for fac in dict_facets_ref[facets[i]]:\n",
    "            dict_facets_new_ref[facets_new[6]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[9]]:\n",
    "            dict_facets_new_ref[facets_new[7]].append(fac)\n",
    "\n",
    "    for fac in dict_facets_ref[facets[12]]:\n",
    "        dict_facets_new_ref[facets_new[8]].append(fac)\n",
    "\n",
    "    return dict_facets_new_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
