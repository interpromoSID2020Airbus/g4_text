{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jan 15 08:50 2020\n",
    "Group 4\n",
    "@authors: Group 4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_top_word_facet(emb_dict: dict, new_facets: list, dict_facets: dict, top_word_df: pd.DataFrame):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters : \n",
    "    emb_dict (dict): word as key and embedding as value\n",
    "    new_facets (list): a list of new facets name\n",
    "    dict_facets (dict): facet name as key and list of top describing words\n",
    "    top_word_df (DataFrame): for each cluster gives the same number of top words\n",
    "\n",
    "    Explanation:\n",
    "    This function computes the cosine distance between each cluster and each facet.\n",
    "    First we create a matrix in which for each top word in our vocabulary we save\n",
    "    the mean of cosine distance with words in the lists describing the facets.\n",
    "    The dataframe's shape is (nb_top_word, nb_facet).\n",
    "    Then for each cluster we mean all the distances of the words describing it.\n",
    "\n",
    "    Out:\n",
    "    DataFrame with all the distance computed, its shape is (nb_cluster, nb_facet)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_word_emb = pd.DataFrame()  # Dataframe with top words of facets at index,\n",
    "    # embeddings dimension as columns plus a column for the facet associated\n",
    "    for key, value in dict_facets.items():\n",
    "        x = []\n",
    "        for word in dict_facets[key]:\n",
    "            x.append(emb_dict[word])\n",
    "        df = pd.DataFrame(data=x, index=dict_facets[key])\n",
    "        df_word_emb = df_word_emb.append(df)\n",
    "        df_word_emb.loc[dict_facets[key], 'facets_label'] = key\n",
    "\n",
    "    # all unique words in the top words lists of clusters\n",
    "    voc = pd.unique(top_word_df.values.ravel('K'))\n",
    "\n",
    "    # Dataframe with words at index and embeddings as columns\n",
    "    word_emb_cluster = pd.DataFrame(\n",
    "        index=[voc], columns=df_word_emb.columns[:-1])\n",
    "    to_drop = []\n",
    "    for word in voc:\n",
    "        try:\n",
    "            word_emb_cluster.loc[word, :] = glove_dict[word]\n",
    "        except:\n",
    "            to_drop += [word]\n",
    "    word_emb_cluster.drop(to_drop, axis=0, inplace=True)\n",
    "\n",
    "    # dataframe with cluster at index and facets's name as columns\n",
    "    # and distance between them as values\n",
    "    # this is the returned object\n",
    "    df_dist = pd.DataFrame(\n",
    "        columns=new_facets, index=range(len(top_word_df))).fillna(0)\n",
    "\n",
    "    # Dataframe with the words in voc at index and facets at columns\n",
    "    # and distance between them as values\n",
    "    word_facet_dist = pd.DataFrame(\n",
    "        columns=new_facets, index=voc).fillna(0).astype(float)\n",
    "\n",
    "    for word in voc:  # calculate distances\n",
    "        array = np.zeros((1, len(df_dist.columns)))\n",
    "        for j in df_word_emb.index:\n",
    "            try:\n",
    "                vect = df_word_emb.loc[j].values[:-1]\n",
    "                facet = df_word_emb.loc[j].values[-1]\n",
    "                word_facet_dist.at[word, facet] += cosine_similarity(\n",
    "                    [vect], word_emb_cluster.loc[word])[0][0]\n",
    "            except:\n",
    "                print('word not found')\n",
    "\n",
    "    # normalize the distance by the number of word in each facet\n",
    "    for facet in word_facet_dist.columns:\n",
    "        word_facet_dist.loc[:, facet] = word_facet_dist.loc[:,\n",
    "                                                            facet] / len(dict_facets[facet])\n",
    "    \n",
    "    # fill the dataframe that is returned\n",
    "    for i in range(len(top_word_df.index)):\n",
    "        array = np.zeros((1, len(df_dist.columns)))\n",
    "        counter = 0\n",
    "        for col in top_word_df.columns:\n",
    "            try:\n",
    "                word = top_word_df.loc[i, col]\n",
    "                array = array + word_facet_dist.loc[word, :].values\n",
    "                counter += 1\n",
    "            except:\n",
    "                print('word not found')\n",
    "\n",
    "        try:\n",
    "            # normalize by the number of word for which we have an embedding\n",
    "            df_dist.loc[i, :] = array[0] / counter\n",
    "        except:\n",
    "            df_dist.loc[i, :] = -1\n",
    "            print('all words not found')\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_facets = ['baggage_facet',\n",
    "    'atmosphere_facet',\n",
    "    'cabin_crew_facet',\n",
    "    'comfort_facet',\n",
    "    'food_facet',\n",
    "    'not_flight_facet',\n",
    "    'price_facet',\n",
    "    'punctuality_facet'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_facets = {\n",
    "    'baggage_facet': ['lose', 'let', 'bag', 'luggage', 'hand', 'people'],\n",
    "    'atmosphere_facet': ['quiet', 'noise', 'calm', 'lounge', 'air'],\n",
    "    'cabin_crew_facet': ['crew', 'cabin', 'helpful', 'professional', 'efficient', 'friendly'],\n",
    "    'comfort_facet': ['seat', 'comfortable', 'legroom', 'room', 'leg', 'space', 'entertainment', 'uncomfortable'],\n",
    "    'food_facet': ['food', 'meal', 'drink', 'snack', 'serve', 'beverage', 'tasty', 'wine', 'eat'],\n",
    "    'not_flight_facet': ['check', 'boarding', 'customer', 'board', 'staff'],\n",
    "    'price_facet': ['price', 'budget', 'cheap', 'cost', 'ticket', 'value', 'pay', 'reasonable'],\n",
    "    'punctuality_facet': ['time', 'delay', 'arrival', 'arrive', 'late', 'schedule', 'departure']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "with open(\"../data/glove.twitter.27B.200d.txt\") as file:\n",
    "    # create a dictionnary that for each word gives its representation in a 200dimentionnal space\n",
    "    for line in file:\n",
    "        values = line.split(' ')\n",
    "        glove_dict[values[0]] = np.array(values[1:]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmi-sid/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:3946: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  new_axis = axis.drop(labels, errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n",
      "word not found\n"
     ]
    }
   ],
   "source": [
    "top_word_df = pd.read_csv('../data/g4_Kmean++_ALL_DATA_ANNOTATE_keyword_cluster.csv', sep=',', index_col=0)\n",
    "\n",
    "distance_top_word_facet(glove_dict, new_facets, dict_facets, top_word_df) # exemple of how to call the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv files with entities that we want\n",
    "airports = pd.read_csv('airport.csv', sep =',')\n",
    "countries = pd.read_csv(\"countries.csv\", sep = ',')\n",
    "cities = pd.read_csv(\"cities.csv\", sep = ',')\n",
    "airlines_data = pd.read_csv('aircraft.csv', sep =',', index_col = 0)\n",
    "aicrafts_data = pd.read_csv('airline.csv', sep =',', index_col = 0)\n",
    "\n",
    "# we only keep the big airports\n",
    "airports = airports[~airports.type.isin(['heliport', 'closed', 'ballonport', 'seaplane_base',\n",
    "                                         'small_airport', 'medimum_airport'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data//ALL_DATA_ANNOTATE.xlsx\") # import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this spacy model to detect entity first\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the entities in list then we export them in a dataframe\n",
    "cities_list = []\n",
    "aircrafts = []\n",
    "airlines = []\n",
    "countries_list = []\n",
    "airports_list = []\n",
    "\n",
    "for i in range(len(data)):  # iterrate over all rows\n",
    "\n",
    "    if comment != comment:\n",
    "        continue\n",
    "    comment = data[\"Sentence\"].loc[i]\n",
    "    # put Capital letters at the beginning of each words\n",
    "    comment_title = comment.title()\n",
    "    # put all the comment in capital letters\n",
    "    comment_upper = comment.upper()\n",
    "\n",
    "    entities = nlp(comment_title)\n",
    "    cities_countries = []\n",
    "    aeroports = []\n",
    "    # iterrate over all entities found by the model\n",
    "    for ent in entities.ents:\n",
    "\n",
    "        if ent.label_ == 'GPE':  # GPE stand for geopolitical entity\n",
    "            cities_countries.append(ent.text)\n",
    "\n",
    "        if ent.label_ == 'FAC': # FAC stands for Facility\n",
    "            aeroports.append(ent.text)\n",
    "    airports_list.append(aeroports)\n",
    "\n",
    "    # lookup in the csv files if the entities are present\n",
    "    city = []\n",
    "    country = []\n",
    "    for entity in cities_countries:\n",
    "        if entity in list(cities[\"name\"]):\n",
    "            city.append(entity)\n",
    "            continue\n",
    "        if entity in list(countries[\"name\"]):\n",
    "            country.append(entity)\n",
    "            continue\n",
    "    cities_list.append(city)\n",
    "    countries_list.append(country)\n",
    "\n",
    "    planes = []\n",
    "    for plane in range(len(aicrafts_data)):\n",
    "        if (aicrafts_data[\"name\"].loc[plane]).upper() in comment_upper:\n",
    "            planes.append(aicrafts_data[\"name\"].loc[plane])\n",
    "    aircrafts.append(planes)\n",
    "\n",
    "    compagnies = []\n",
    "    for compagnie in range(len(airlines_data)):\n",
    "        if (airlines_data[\"name\"].loc[compagnie]).upper() in comment_upper:\n",
    "            compagnies.append(airlines_data[\"name\"].loc[compagnie])\n",
    "    airlines.append(compagnies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock the results in a dataframe\n",
    "pd.DataFrame(data=np.array([airports_list, cities_list, countries_list, aircrafts, airlines]).T,\n",
    "             columns=['airport', 'city', 'country', 'aircraft', 'airlines']).to_csv('../Output/entite.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
